/*
 * Molecule Fragmentation Nextflow process:
 * Purpose: Fragmentation of molecules across a cluster.
 *
 * Author | Date    | Version
 * Tim    | 03/2020 | Initial Version
 * Duncan | 03/2020 | Add comtainer to collect* processes.
 * Alan   | 12/2024 | Convert to DSL 2
 *
 */

nextflow.enable.dsl=2

params.input = 'nonisomol.smi'
params.chunk_size = 500000
params.max_hac = 36
params.max_frag = 12
params.out_dir = '.'
params.out_mode = 'move'
params.tmp_dir = '.'

chunks = Channel.from(file(params.input))
    .splitText(by: params.chunk_size, file: 'chunk_')

process fragment_mols {

{% if nextflow_container_registry %}
    container '{{ nextflow_container_registry }}/{{ nextflow_container_name }}:{{ nextflow_container_tag }}'
{% else %}
    container '{{ nextflow_container_name }}:{{ nextflow_container_tag }}'
{% endif %}
    errorStrategy = 'retry'
    maxErrors = 3

    input:
    file chunks

    output:
    path('nodes_*.csv')
    path('edges_*.csv')
    path('rejects_*.smi'), optional: true

    """
    python -m frag.network.scripts.build_db_from_smiles --input $chunks --base_dir ./ --max-frag ${params.max_frag} --max-hac ${params.max_hac}
    mv nodes.csv nodes_${chunks}.csv
    mv edges.csv edges_${chunks}.csv
    if [ -f rejects.smi ]; then mv rejects.smi rejects_${chunks}.smi; fi
    """
}

process collect_nodes {

{% if nextflow_container_registry %}
    container '{{ nextflow_container_registry }}/{{ nextflow_container_name }}:{{ nextflow_container_tag }}'
{% else %}
    container '{{ nextflow_container_name }}:{{ nextflow_container_tag }}'
{% endif %}
    memory '{{ nextflow_process_sort_memory_m }} MB'
    publishDir params.out_dir, mode: params.out_mode
    errorStrategy = 'retry'
    maxErrors = 3

    input:
    file chunks

    output:
    path 'nodes.csv'

    """
    sort -u -S {{ (nextflow_process_sort_memory_m * 0.8)|int }}M -T . $chunks > nodes.csv
    """
}

process collect_edges {

{% if nextflow_container_registry %}
    container '{{ nextflow_container_registry }}/{{ nextflow_container_name }}:{{ nextflow_container_tag }}'
{% else %}
    container '{{ nextflow_container_name }}:{{ nextflow_container_tag }}'
{% endif %}
    memory '{{ nextflow_process_sort_memory_m }} MB'
    publishDir params.out_dir, mode: params.out_mode
    errorStrategy = 'retry'
    maxErrors = 3

    input:
    file chunks

    output:
    path 'edges.csv'

    """
    sort -u -S {{ (nextflow_process_sort_memory_m * 0.8)|int }}M -T . $chunks > edges.csv
    """
}

process collect_rejects {

{% if nextflow_container_registry %}
    container '{{ nextflow_container_registry }}/{{ nextflow_container_name }}:{{ nextflow_container_tag }}'
{% else %}
    container '{{ nextflow_container_name }}:{{ nextflow_container_tag }}'
{% endif %}
    memory '{{ nextflow_process_sort_memory_m }} MB'
    publishDir params.out_dir, mode: params.out_mode
    errorStrategy = 'retry'
    maxErrors = 3

    input:
    file chunks

    output:
    path 'rejects.smi'

    """
    sort -u -S {{ (nextflow_process_sort_memory_m * 0.8)|int }}M -T . $chunks > rejects.smi
    """
}

// workflow definitions
workflow fragment {
    take:
    file chunks

    main:
    fragment_mols(chunks)
    collect_nodes(fragment_mols.out[0].collect())
    collect_edges(fragment_mols.out[1].collect())
    if (fragment_mols.out[2].collect()) {
        collect_rejects(fragment_mols.out[2].collect())
    }
}

workflow {
    fragment(chunks)
}
