---

# The assumption here is that an extract has been done and
# a number of compressed (.gz) node and edge (relationship) files exist
# in the {{ copy_root }}/extract directory. The number of files will be
# dependent on the extract - the actual number is unimportant - the essential
# property of the extracted files is that their file-names contain the text
# 'nodes' or 'edges' (but not both) and they end '.gz'.
#
# Importantly, every '.gz' file we expect to find must have a corresponding
# 'header-' file in the role's 'files' directory. So, if there's an 'x.csv.gz'
# in the extract there must be a 'header-x.csv' in 'files'.
#
# The tasks in this play are responsible for: -
#
# 1. compressing the extract
# 2. templating the loader script (load-neo4j.sh) based on the files found
# 3. installing the graph header files (located in the role's 'files')
# 4. synchronising the files to AWS S3

# Pack all files to '.gz' files -1 should compress more quickly.
# We run this on (delegate to) the DB server...

- name: Compress extracted csv files
  shell: gzip *.csv
  args:
    chdir: "{{ copy_root }}/extract"

# List all the node files into ' node-files.txt'.
# This is a 1-file-per-line list of extracted nodes.

- name: Collect node files
  shell: ls -1 *nodes*.gz > node-files.txt
  args:
    chdir: "{{ copy_root }}/extract"

# Create the 'node_text' variable that will be used when templating
# the loader script. This is essentially a '--nodes' directive for
# each line in 'node-files.txt'. If 'node-files.txt'
# is: -
#
#    nodes.csv.gz
#    isomol-nodes.csv.gz
#
# Then the 'nodes' variable should look like this
# (with each line indented by 8 spaces)...
#
#    --nodes "header-nodes.csv,nodes.csv.gz" \
#    --nodes "header-isomol-nodes.csv,isomol-nodes.csv.gz" \

- name: Form node_result
  shell: >-
    paste -d , node-files.txt node-files.txt
    | sed 's/.gz,/,/g'
    | sed 's/^/        --nodes "header-/g'
    | sed 's/$/" \\/g'
  register: node_result
  args:
    chdir: "{{ copy_root }}/extract"

- name: Set node_text
  set_fact:
    node_text: "{{ node_result.stdout }}"

# List all the edge files into 'edge-files.txt'.
# This is a 1-file-per-line list of extracted edges.

- name: Collect edge files
  shell: ls -1 *edges*.gz > edge-files.txt
  args:
    chdir: "{{ copy_root }}/extract"

- name: Form relationship_result
  shell: >-
    paste -d , edge-files.txt edge-files.txt
    | sed 's/.gz,/,/g'
    | sed 's/^/        --relationships "header-/g'
    | sed 's/$/" \\/g'
    | sed '$ s/.$//'
  register: relationship_result
  args:
    chdir: "{{ copy_root }}/extract"

- name: Set relationship_text
  set_fact:
    relationship_text: "{{ relationship_result.stdout }}"

# Now template the script using the node & relationship variables
# and copy all the header files we have
# (some may not be used - just copy them all anyway)...

- name: Install the loader script
  template:
    src: load-neo4j.sh.j2
    dest: "{{ copy_root }}/extract/load-neo4j.sh"
    mode: 0555

- name: Copy header files
  copy:
    src: "{{ item }}"
    dest: "{{ copy_root }}/extract/{{ item|basename }}"
    mode: 0444
  with_fileglob:
  - header-*

- name: Remove node and edge text files
  command: rm node-files.txt edge-files.txt
  args:
    chdir: "{{ copy_root }}/extract"
  become_user: "{{ database[deployment].db_user_account }}"

- name: List files
  find:
    paths: "{{ copy_root }}/extract"
  register: find_result

- name: Extract files
  set_fact:
    files_found: "{{ find_result|json_query('files[*].path')|flatten }}"

- name: Display files
  debug:
    var: files_found

# Synchronise the extract to S3 if desired,
# otherwise this data is left in the extract directory on the server.

- name: Transfer files to S3
  block:

  # When only one vendor, use it
  - name: Single Vendor
    block:

    - name: Set bucket object path
      set_fact:
        s3_object_path: "extract/{{ extracts[0].lib.vendor }}/{{ extracts[0].lib.version }}"

    - name: Display bucket object path
      debug:
        var: s3_object_path

    - name: Transfer files (AWS) - Single Vendor
      aws_s3:
        bucket: "{{ s3_bucket }}"
        encrypt: "{{ s3_bucket_requires_encryption|bool }}"
        src: "{{ item }}"
        object: "{{ s3_object_path }}/{{ item|basename }}"
        mode: put
      loop: "{{ files_found }}"
      when: s3_url|length == 0

    - name: Transfer files (non-AWS) - Single Vendor
      aws_s3:
        bucket: "{{ s3_bucket }}"
        encrypt: "{{ s3_bucket_requires_encryption|bool }}"
        src: "{{ item }}"
        object: "{{ s3_object_path }}/{{ item|basename }}"
        mode: put
        s3_url: "{{ s3_url }}"
        aws_access_key: "{{ s3_access_key }}"
        aws_secret_key: "{{ s3_secret_key }}"
        region: "{{ s3_region }}"
      loop: "{{ files_found }}"
      when: s3_url|length > 0

    when: source_id_lst|length == 1

  # When combination, use combination/first vendor/first version
  - name: Combination
    block:

    - name: Set bucket object path
      set_fact:
        s3_object_path: "extract/combination/{{ extracts[0].lib.vendor }}/{{ ansible_date_time.date }}"

    - name: Display bucket object path
      debug:
        var: s3_object_path

    - name: Transfer files (AWS) - Combination
      aws_s3:
        bucket: "{{ s3_bucket }}"
        encrypt: "{{ s3_bucket_requires_encryption|bool }}"
        src: "{{ item }}"
        object: "{{ s3_object_path }}/{{ item|basename }}"
        mode: put
      loop: "{{ files_found }}"
      when: s3_url|length == 0

    - name: Transfer files (non-AWS) - Combination
      aws_s3:
        bucket: "{{ s3_bucket }}"
        encrypt: "{{ s3_bucket_requires_encryption|bool }}"
        src: "{{ item }}"
        object: "{{ s3_object_path }}/{{ item|basename }}"
        mode: put
        s3_url: "{{ s3_url }}"
        aws_access_key: "{{ s3_access_key }}"
        aws_secret_key: "{{ s3_secret_key }}"
        region: "{{ s3_region }}"
      loop: "{{ files_found }}"
      when: s3_url|length > 0

    when: source_id_lst|length > 1

  when: data_source[deployment]=="s3"
