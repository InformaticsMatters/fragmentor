---

# Variables that a user might want to change
# through the command-line or a parameter file.

# Neo4j Extract Parameters

# Nodes and Edges Neo4j Files loaded from database.
supnodefile: supplier-nodes.csv
supmolnodefile: suppliermol-nodes.csv
isosupmoledgefile: isomol-suppliermol-edges.csv
supmolsupedgefile: suppliermol-supplier-edges.csv
molsupmoledgefile: molecule-suppliermol-edges.csv

neonodefile: nodes.csv
isomolnodefile: isomol-nodes.csv

neoedgefile: edges.csv
inchiidfile: inchi-nodes.csv
molinchiedgefile: molecule-inchi-edges.csv
isomoledgefile: isomol-molecule-edges.csv

# Processing definition
# These lists define which files will be processed by which method.
# File names will be identfied by their extract index 0,1,2 and their file name when copied to the work volume.
# E.g. nodes.csv from extract [0] will be
concat: [ '{{ supnodefile }}', '{{ supmolnodefile }}', '{{ isosupmoledgefile }}', '{{ supmolsupedgefile }}', '{{ molsupmoledgefile }}' ]
merge: ['{{ neonodefile }}', '{{ isomolnodefile }}']
deduplicate: [ '{{ neoedgefile }}', '{{ inchiidfile }}', '{{ molinchiedgefile }}', '{{ isomoledgefile }}' ]

# Shatterfile control. It only makes sense to use the complete cluster for very large files.
# Otherwise the overhead of using the cluster is too great.
# The code does a workcount on the input files. If linecount > threshold use whole cluster.
shatterfiles_small: 80
shatterfiles_large: "{{ hardware[deployment].parallel_jobs }}"
shatterfile_large_threshold_kb: 1048576

# The maximum execution time
# and poll period for the (asynchronous) Nextflow process
# Timeouts (minutes):  1440 (24 hours)
nextflow_timeout_minutes: 1440
nextflow_poll_period_seconds: 30

compress_timeout_minutes: 1440
compress_poll_period_minutes: 1

# Results of compress commands.
compress_tasks: []
