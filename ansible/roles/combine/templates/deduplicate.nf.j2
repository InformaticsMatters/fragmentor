// Fragalysis Graph Deduplication Processing
//
// This workflow de-duplicates the 'combined' node and edge files.
// by first 'shattering' them over a number of files before sorting them
// in parallel to and re-combing them into a grand nodes and edges file.
//
// There are a number of chained processes here. Briefly: -
//
// - eShatter : Shatters the edge files to be combined into
//              files based on their hash
// - eDedup   : De-duplicates each edge-shattered file ('shard')
// - eCat     : Concatenates the deduplicated edge shards
//              to form a _grand_ 'all-nodes.csv'

params.shatterFiles = 80
edges = Channel.fromPath("./*-edges.csv.gz")

// Shatter (all) fragment edge files into a hash-based of files.
// -------
// The shattering is based on the individual line's hash
// so potential duplicates are guaranteed to be written
// to the same file. The output of this process is
// a number of files named 'n-shatter-NNN.csv'.
process eShatter {

    container 'informaticsmatters/fragalysis:{{ fragalysis_version }}'
    publishDir 'results/', mode: 'symlink'
    scratch false

    input:
    file edge from edges.toList()

    output:
    file 'e-shatter-*.csv' into shattered_edges mode flatten

    """
    python /usr/local/fragalysis/frag/network/scripts/csv_shatter.py \
        . edges.csv.gz ${params.shatterFiles} e-shatter \
        --delete-input
    """

}

// Edge deduplication
// ------------------
// Here we process a small (shattered) file.
// If a node has a duplicate it will reside in the same shattered file.
// We ensure that cpus equals the number of cores on a node,
// so we get an entire node.
process eDedup {

    container 'informaticsmatters/fragalysis:{{ fragalysis_version }}'
    cpus {{ cluster_cores_per_node }}
    publishDir 'results/', mode: 'symlink'
    scratch false

    input:
    file shard from shattered_edges

    output:
    file '*.d-edges' into dedup_edges

    shell:
    '''
    sort --temporary-directory=. -u !{shard} > !{shard}.d-edges
    rm !{shard}
    '''

}

// Concatenate the deduplicated edges
// -----------
// This results in a 'grand' edge deduplication file 'all-edges.csv'.
// See 'nConcat' process above.
//
// As 'eCat' is the last process (for Edges) in this workflow
// we publish by moving the output file in order to save valuable time
// with a potentially very large file.
process eCat {

    container 'informaticsmatters/fragalysis:{{ fragalysis_version }}'
    publishDir 'results/', mode: 'move'
    scratch false

    input:
    file dedup from dedup_edges.toList()

    output:
    file 'all-edges.csv'

    """
    find . -name '*.d-edges' | xargs cat > all-edges.csv
    find . -name '*.d-edges' -delete
    """

}
