---
# Gets 'raw' data from an AWS S3 path
# 1. Goes to the repo listed in the 'path' variable on disk and copies files to data directory (if not already present)
# 2. Each file in the s3 bucket should be copied to the work_path with the file prepended by the index.

- name: Variable sanity-check
  assert:
    that:
    - "{{ item.lib.path|string|length > 0 }}"

- name: Display extract path
  debug:
    msg: bucket={{ bucket }} extract_path={{ item.lib.path }}

- name: Create data directory
  file:
    path: "{{ data_path }}/{{ item.lib.path }}"
    state: directory

# But don't get the files (which may be large)
# if the data files appear to already exist.

- name: Check if nodes file is already on work path
  stat:
    path: "{{ work_path }}/{{ extract_index }}-nodes.csv.gz"
  register: nodes_gz_data_file
  changed_when: false

- name: List extract data
  aws_s3:
    bucket: "{{ bucket }}"
    mode: list
    prefix: "{{ item.lib.path }}"
    aws_access_key: "{{ aws_access_key }}"
    aws_secret_key: "{{ aws_secret_key }}"
  register: extract_result
  when:
  - not nodes_gz_data_file.stat.exists

- name: Display result
  debug:
    var: extract_result
  when:
  - not nodes_gz_data_file.stat.exists

# There has to be at least two keys in the result...
# The first key is always the directory the files are in,
# e.g.: "s3_keys": [
#           "raw/vendor/xchem/v1/",
#           "raw/vendor/xchem/v1/dsip.txt.gz"
#        ]

- name: Check extract data
  assert:
    that: extract_result.s3_keys|length > 1
  when:
  - not nodes_gz_data_file.stat.exists

- name: Display extract file count
  debug:
    msg: Found {{ extract_result.s3_keys|length - 1 }} extract files
  when:
  - not nodes_gz_data_file.stat.exists

- name: Set destination path
  set_fact:
    destination: "{{ data_path }}/{{ item.lib.path }}"
  when:
  - not nodes_gz_data_file.stat.exists

# Get the files, ignoring the first entry in the list (always a directory)...

- name: Get extract data
  aws_s3:
    bucket: "{{ bucket }}"
    mode: get
    object: "{{ file_item }}"
    dest: "{{ destination }}/{{ file_item|basename }}"
    aws_access_key: "{{ aws_access_key }}"
    aws_secret_key: "{{ aws_secret_key }}"
  loop: "{{ extract_result.s3_keys[1:] }}"
  loop_control:
    loop_var: file_item
  when:
  - not nodes_gz_data_file.stat.exists

# Change the name and move it to the common work directory
- name: Move to work directory
  command: cp {{ file_item }} {{ work_path }}/{{ extract_index }}-{{ file_item | basename }}
  with_fileglob:
  - "{{  data_path }}//{{ item.lib.path }}/*.gz"
  loop_control:
    loop_var: file_item
  when:
  - not nodes_gz_data_file.stat.exists



