---
# Initialise Fragmentation Process

- debug:
    msg: "{{ reppath }}/run/vendor/{{ vendor }}"

# Install the modules to be able to synchronise the data from S3

- name: Upgrade PIP
  command: pip install --upgrade pip

- name: Install S3 requirements
  pip:
    name:
    - python-dateutil==2.8.1
    - boto3==1.12.49

# Check the vendor name is one that can be processed.
- name: check vendor_name
  postgresql_query:
    query: >-
      select * from vendor_name
      where vendor_name = '{{ vendor }}';
    autocommit: no

# Check if the data library version for the vendor has been processed before.
- name: check if previous version
  postgresql_query:
    query: select id from source where name = '{{ vendor }}' and version = '{{ version }}';
    autocommit: no
  register: query_result

- debug:
    var: query_result

# If add_file is no (normal situation) then check the version had not been processed before
- name: version already processed sanity-check
  assert:
    that:
    - "{{ query_result.rowcount|int }} == 0"
  when: not add_file|bool

# If add_file is true then use the existing source id from the previous run
- name: Set source_id
  set_fact:
    source_id: "{{ query_result.query_result[0].id }}"
  when: add_file|bool

- name: Variable sanity-check
  assert:
    that:
    - "{{ vendors[vendor].fraghac|int }} > 0"
    - "{{ vendors[vendor].fragminhac|int }} >= 0"
    - "{{ vendors[vendor].fragmaxfrags|int }} > 0"
    - "{{ vendors[vendor].fraglimit|int }} >= 0"

# If add_file is no (normal situation) and the version has not been processed then add to the source table.
- name: Insert run parameters
  postgresql_query:
    query: >-
      insert into source (name, version, min_hac, max_hac, max_frags, frag_limit, start_datetime )
      values ('{{ vendor }}', '{{ version }}',{{ vendors[vendor].fragminhac|int }},
              {{ vendors[vendor].fraghac|int }},{{ vendors[vendor].fragmaxfrags|int }}, {{ vendors[vendor].fraglimit|int }}, now() at time zone 'utc')
      returning id;
    autocommit: yes
  register: query_result
  when: not add_file|bool

# If add_file is true then use the existing source id from the previous run
- name: Set source_id
  set_fact:
    source_id: "{{ query_result.query_result[0].id }}"
  when: not add_file|bool

- name: Display source_id
  debug:
    var: source_id

# Optionally erase and then create the working directories...
- name: Erase directories
  file:
    path: "{{ item }}"
    state: absent
  loop:
  - "{{ datapath }}/data/{{ vendor }}"
  - "{{ standpath }}/standardise"
  - "{{ logpath }}/{{ vendor }}/{{ version }}/standardise"
  when: clean_start|bool

- name: Create directories
  file:
    path: "{{ item }}"
    state: directory
  loop:
  - "{{ datapath }}/data/{{ vendor }}"
  - "{{ standpath }}/standardise"
  - "{{ logpath }}/log/{{ vendor }}/{{ version }}/standardise"

- name: Ensure sql and nextflow directories are present
  file:
    path: "{{ item }}"
    state: directory
  loop:
  - "{{ reppath }}/sql"
  - "{{ reppath }}/nextflow"

# Create Standardisation Summary Report. This is saved with the playbook results.
- name: Create Standardise HTML log report
  blockinfile:
    path: "{{ standpath }}/standardise/standardise_report.html"
    create: yes
    marker: "<!-- {standardise-init} ANSIBLE MANAGED BLOCK -->"
    insertafter: "<body>"
    block: |
      <h1>Ansible Standardisation Playbook</h1>
      <p><table>
      <tr><th>Parameter</th><th>Value</th></tr>
      <tr><td>Vendor</td><td>{{ vendor }}</td></tr>
      <tr><td>Version</td><td>{{ version }}</td></tr>
      <tr><td>Source-id</td><td>{{ source_id }}</td></tr>
      <tr><td>User</td><td>{{ ansible_hostname }}</td></tr>
      <tr><td>Deployment</td><td>{{ deployment }}</td></tr>
      </table></p>
      <p>Standardisation play started on: {{ now(utc=True).isoformat() }}</p>
  when: clean_start|bool
